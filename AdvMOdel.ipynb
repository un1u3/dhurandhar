{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140ae273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83bbcd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f085ca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q kaggle opendatasets pillow matplotlib seaborn scikit-learn\n",
    "!pip install -q pytorch-grad-cam timm albumentations\n",
    "!pip install -q wandb tensorboard  # Experiment tracking\n",
    "!pip install -q grad-cam opencv-python\n",
    "!pip install -q efficientnet-pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a68fcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import shutil\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a8ec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                             roc_auc_score, roc_curve, precision_recall_curve,\n",
    "                             f1_score, accuracy_score)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Albumentations for advanced augmentation\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Model interpretability\n",
    "from pytorch_grad_cam import GradCAM, GradCAMPlusPlus\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "\n",
    "print(f\" PyTorch: {torch.__version__} | CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\" GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\" GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7ce044",
   "metadata": {},
   "outputs": [],
   "source": [
    " ========================================== \n",
    "# SECTION 3: CONFIGURATION & HYPERPARAMETERS\n",
    "# ==========================================\n",
    "class Config:\n",
    "    # Paths\n",
    "    BASE_DIR = '/content/xray_dataset'\n",
    "    CHECKPOINT_DIR = '/content/checkpoints'\n",
    "    LOG_DIR = '/content/logs'\n",
    "    \n",
    "    # Model\n",
    "    MODEL_NAME = 'efficientnet_b3'  # Options: efficientnet_b0-b7, resnet50, densenet121\n",
    "    NUM_CLASSES = 4\n",
    "    IMG_SIZE = 256  # Increased for better feature extraction\n",
    "    PRETRAINED = True\n",
    "    \n",
    "    # Training\n",
    "    BATCH_SIZE = 24\n",
    "    NUM_EPOCHS = 25\n",
    "    LEARNING_RATE = 3e-4\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    WARMUP_EPOCHS = 3\n",
    "    \n",
    "    # Augmentation\n",
    "    USE_ADVANCED_AUG = True\n",
    "    MIXUP_ALPHA = 0.2\n",
    "    CUTMIX_ALPHA = 1.0\n",
    "    \n",
    "    # Optimization\n",
    "    USE_MIXED_PRECISION = True\n",
    "    GRADIENT_CLIP = 1.0\n",
    "    EARLY_STOPPING_PATIENCE = 7\n",
    "    \n",
    "    # Validation\n",
    "    K_FOLDS = 5\n",
    "    TEST_TIME_AUGMENTATION = True\n",
    "    TTA_ITERATIONS = 5\n",
    "    \n",
    "    # Classes\n",
    "    CLASSES = ['NORMAL', 'PNEUMONIA', 'COVID', 'TB']\n",
    "    \n",
    "    # Random seed\n",
    "    SEED = 42\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(Config.CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(Config.LOG_DIR, exist_ok=True)\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(Config.SEED)\n",
    "print(\" Configuration loaded & seeds set!\")\n",
    "\n",
    "# ========================================== \n",
    "# SECTION 4: EXPERIMENT LOGGER\n",
    "# ==========================================\n",
    "class ExperimentLogger:\n",
    "    def __init__(self, log_dir):\n",
    "        self.log_dir = log_dir\n",
    "        self.metrics = defaultdict(list)\n",
    "        self.start_time = time.time()\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.log_file = os.path.join(log_dir, f'experiment_{timestamp}.json')\n",
    "        \n",
    "    def log(self, phase, epoch, metrics_dict):\n",
    "        \"\"\"Log metrics for an epoch\"\"\"\n",
    "        metrics_dict['epoch'] = epoch\n",
    "        metrics_dict['phase'] = phase\n",
    "        metrics_dict['timestamp'] = time.time() - self.start_time\n",
    "        \n",
    "        for key, value in metrics_dict.items():\n",
    "            self.metrics[f\"{phase}_{key}\"].append(value)\n",
    "        \n",
    "        # Save to file\n",
    "        with open(self.log_file, 'w') as f:\n",
    "            json.dump(dict(self.metrics), f, indent=2)\n",
    "    \n",
    "    def get_best_metric(self, metric_name):\n",
    "        \"\"\"Get best value of a metric\"\"\"\n",
    "        values = self.metrics.get(metric_name, [])\n",
    "        return max(values) if values else 0.0\n",
    "\n",
    "logger = ExperimentLogger(Config.LOG_DIR)\n",
    "print(f\" Logger initialized: {logger.log_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950c6f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 5: DATASET WITH ADVANCED FEATURES\n",
    "# ==========================================\n",
    "# Download datasets (same as before, condensed)\n",
    "import opendatasets as od\n",
    "print(\" Downloading datasets...\")\n",
    "datasets = [\n",
    "    \"https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia\",\n",
    "    \"https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database\",\n",
    "    \"https://www.kaggle.com/datasets/raddar/tuberculosis-chest-xrays-shenzhen\"\n",
    "]\n",
    "for dataset in datasets:\n",
    "    try:\n",
    "        od.download(dataset)\n",
    "    except:\n",
    "        print(f\"âš ï¸ Failed to download {dataset}\")\n",
    "\n",
    "# Organize dataset\n",
    "def organize_dataset():\n",
    "    base_dir = Config.BASE_DIR\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        for category in Config.CLASSES:\n",
    "            os.makedirs(f'{base_dir}/{split}/{category}', exist_ok=True)\n",
    "    \n",
    "    def move_images(source_paths, label, ratios=(0.7, 0.15, 0.15)):\n",
    "        all_images = []\n",
    "        for path in source_paths:\n",
    "            if os.path.exists(path):\n",
    "                imgs = [os.path.join(path, f) for f in os.listdir(path) \n",
    "                       if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "                all_images.extend(imgs)\n",
    "        \n",
    "        np.random.shuffle(all_images)\n",
    "        n1 = int(len(all_images) * ratios[0])\n",
    "        n2 = int(len(all_images) * ratios[1])\n",
    "        \n",
    "        splits = {\n",
    "            'train': all_images[:n1],\n",
    "            'val': all_images[n1:n1+n2],\n",
    "            'test': all_images[n1+n2:]\n",
    "        }\n",
    "        \n",
    "        for split, imgs in splits.items():\n",
    "            dest = f'{base_dir}/{split}/{label}'\n",
    "            for i, img in enumerate(tqdm(imgs, desc=f'{label} {split}')):\n",
    "                try:\n",
    "                    shutil.copy(img, f'{dest}/{label}_{i}.jpg')\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        return len(splits['train']), len(splits['val']), len(splits['test'])\n",
    "    \n",
    "    # Organize all classes\n",
    "    stats = {}\n",
    "    stats['NORMAL'] = move_images([\n",
    "        '/content/chest-xray-pneumonia/chest_xray/train/NORMAL',\n",
    "        '/content/chest-xray-pneumonia/chest_xray/test/NORMAL'\n",
    "    ], 'NORMAL')\n",
    "    \n",
    "    stats['PNEUMONIA'] = move_images([\n",
    "        '/content/chest-xray-pneumonia/chest_xray/train/PNEUMONIA',\n",
    "        '/content/chest-xray-pneumonia/chest_xray/test/PNEUMONIA'\n",
    "    ], 'PNEUMONIA')\n",
    "    \n",
    "    stats['COVID'] = move_images([\n",
    "        '/content/covid19-radiography-database/COVID-19_Radiography_Dataset/COVID/images'\n",
    "    ], 'COVID')\n",
    "    \n",
    "    stats['TB'] = move_images([\n",
    "        '/content/tuberculosis-chest-xrays-shenzhen/images'\n",
    "    ], 'TB')\n",
    "    \n",
    "    print(\"\\n Dataset Statistics:\")\n",
    "    print(pd.DataFrame(stats, index=['Train', 'Val', 'Test']))\n",
    "    return stats\n",
    "\n",
    "dataset_stats = organize_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfe79b5",
   "metadata": {},
   "outputs": [],
   "source": [
    " ========================================== \n",
    "# SECTION 6: ADVANCED AUGMENTATION PIPELINE\n",
    "# ==========================================\n",
    "def get_train_transforms(img_size=256):\n",
    "    \"\"\"Advanced augmentation with Albumentations\"\"\"\n",
    "    return A.Compose([\n",
    "        A.Resize(img_size, img_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.15, rotate_limit=15, p=0.5),\n",
    "        A.OneOf([\n",
    "            A.ElasticTransform(alpha=120, sigma=120 * 0.05, p=1),\n",
    "            A.GridDistortion(p=1),\n",
    "            A.OpticalDistortion(distort_limit=1, shift_limit=0.5, p=1),\n",
    "        ], p=0.3),\n",
    "        A.OneOf([\n",
    "            A.GaussNoise(var_limit=(10.0, 50.0), p=1),\n",
    "            A.GaussianBlur(blur_limit=3, p=1),\n",
    "            A.MotionBlur(blur_limit=3, p=1),\n",
    "        ], p=0.2),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.3),\n",
    "        A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.3),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "def get_val_transforms(img_size=256):\n",
    "    \"\"\"Validation transforms\"\"\"\n",
    "    return A.Compose([\n",
    "        A.Resize(img_size, img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d918e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================== \n",
    "# SECTION 7: CUSTOM DATASET WITH CACHING\n",
    "# ==========================================\n",
    "class AdvancedXRayDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', transform=None, cache=True):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.classes = Config.CLASSES\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        self.cache = cache\n",
    "        self.cache_dict = {}\n",
    "        \n",
    "        # Load all paths\n",
    "        self.samples = []\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(root_dir, split, class_name)\n",
    "            if os.path.exists(class_dir):\n",
    "                for img_name in os.listdir(class_dir):\n",
    "                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        path = os.path.join(class_dir, img_name)\n",
    "                        self.samples.append((path, self.class_to_idx[class_name]))\n",
    "        # ========================================== \n",
    "# SECTION 7: CUSTOM DATASET WITH CACHING\n",
    "# ==========================================\n",
    "class AdvancedXRayDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', transform=None, cache=True):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.classes = Config.CLASSES\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        self.cache = cache\n",
    "        self.cache_dict = {}\n",
    "        \n",
    "        # Load all paths\n",
    "        self.samples = []\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(root_dir, split, class_name)\n",
    "            if os.path.exists(class_dir):\n",
    "                for img_name in os.listdir(class_dir):\n",
    "                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        path = os.path.join(class_dir, img_name)\n",
    "                        self.samples.append((path, self.class_to_idx[class_name]))\n",
    "        \n",
    "        print(f\"Loaded {len(self.samples)} images for {split} set\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        # Check cache\n",
    "        if self.cache and img_path in self.cache_dict:\n",
    "            image = self.cache_dict[img_path]\n",
    "        else:\n",
    "            image = cv2.imread(img_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            if self.cache:\n",
    "                self.cache_dict[img_path] = image\n",
    "        \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        \n",
    "        return image, label\n",
    "        print(f\"Loaded {len(self.samples)} images for {split} set\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        # Check cache\n",
    "        if self.cache and img_path in self.cache_dict:\n",
    "            image = self.cache_dict[img_path]\n",
    "        else:\n",
    "            image = cv2.imread(img_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            if self.cache:\n",
    "                self.cache_dict[img_path] = image\n",
    "        \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1735092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================================== \n",
    "# SECTION 8: MULTI-MODEL ARCHITECTURE\n",
    "# ==========================================\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"Channel Attention Module\"\"\"\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class AdvancedXRayModel(nn.Module):\n",
    "    def __init__(self, model_name='efficientnet_b3', num_classes=4, pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load backbone\n",
    "        if 'efficientnet' in model_name:\n",
    "            if model_name == 'efficientnet_b0':\n",
    "                self.backbone = models.efficientnet_b0(pretrained=pretrained)\n",
    "                num_features = 1280\n",
    "            elif model_name == 'efficientnet_b3':\n",
    "                self.backbone = models.efficientnet_b3(pretrained=pretrained)\n",
    "                num_features = 1536\n",
    "            else:\n",
    "                raise ValueError(f\"Model {model_name} not supported\")\n",
    "            \n",
    "            # Add attention before classifier\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "            self.attention = AttentionBlock(num_features)\n",
    "            \n",
    "        elif model_name == 'resnet50':\n",
    "            self.backbone = models.resnet50(pretrained=pretrained)\n",
    "            num_features = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "            self.attention = AttentionBlock(num_features)\n",
    "        \n",
    "        # Custom classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # If features are 4D, apply attention\n",
    "        if len(features.shape) == 4:\n",
    "            features = self.attention(features)\n",
    "            features = F.adaptive_avg_pool2d(features, 1).flatten(1)\n",
    "        \n",
    "        return self.classifier(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b667a96",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'venv (Python 3.12.3)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/home/unique/Desktop/dhurandhar/venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "# ========================================== \n",
    "# SECTION 9: ADVANCED TRAINING UTILITIES\n",
    "# ==========================================\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    \"\"\"Mixup augmentation\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    \n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    \n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"Mixup loss\"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for handling class imbalance\"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        \n",
    "        if self.alpha is not None:\n",
    "            focal_loss = self.alpha[targets] * focal_loss\n",
    "            \n",
    "        return focal_loss.mean()\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
    "    def __init__(self, patience=7, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.should_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.should_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        \n",
    "        return self.should_stop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b153a7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================== \n",
    "# SECTION 10: ENHANCED TRAINING LOOP\n",
    "# ==========================================\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, config):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.config = config\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # Loss & optimizer\n",
    "        class_counts = [sum(1 for _, l in train_loader.dataset if l == i) \n",
    "                       for i in range(config.NUM_CLASSES)]\n",
    "        weights = torch.FloatTensor([1.0 / max(c, 1) for c in class_counts])\n",
    "        weights = weights / weights.sum() * config.NUM_CLASSES\n",
    "        self.criterion = FocalLoss(alpha=weights.to(self.device), gamma=2.0)\n",
    "        \n",
    "        self.optimizer = optim.AdamW(model.parameters(), \n",
    "                                     lr=config.LEARNING_RATE,\n",
    "                                     weight_decay=config.WEIGHT_DECAY)\n",
    "        \n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            self.optimizer, T_0=10, T_mult=2, eta_min=1e-6\n",
    "        )\n",
    "        \n",
    "        # Mixed precision\n",
    "        self.scaler = GradScaler() if config.USE_MIXED_PRECISION else None\n",
    "        \n",
    "        # Early stopping\n",
    "        self.early_stopping = EarlyStopping(patience=config.EARLY_STOPPING_PATIENCE)\n",
    "        \n",
    "        self.best_val_acc = 0.0\n",
    "        self.history = defaultdict(list)\n",
    "        \n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}/{self.config.NUM_EPOCHS}')\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(pbar):\n",
    "            images, labels = images.to(self.device), labels.to(self.device)\n",
    "            \n",
    "            # Mixup augmentation\n",
    "            if self.config.USE_ADVANCED_AUG and np.random.rand() < 0.5:\n",
    "                images, labels_a, labels_b, lam = mixup_data(\n",
    "                    images, labels, self.config.MIXUP_ALPHA\n",
    "                )\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                if self.scaler:\n",
    "                    with autocast():\n",
    "                        outputs = self.model(images)\n",
    "                        loss = mixup_criterion(self.criterion, outputs, \n",
    "                                             labels_a, labels_b, lam)\n",
    "                    self.scaler.scale(loss).backward()\n",
    "                    self.scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), \n",
    "                                                  self.config.GRADIENT_CLIP)\n",
    "                    self.scaler.step(self.optimizer)\n",
    "                    self.scaler.update()\n",
    "                else:\n",
    "                    outputs = self.model(images)\n",
    "                    loss = mixup_criterion(self.criterion, outputs, \n",
    "                                         labels_a, labels_b, lam)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), \n",
    "                                                  self.config.GRADIENT_CLIP)\n",
    "                    self.optimizer.step()\n",
    "            else:\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                if self.scaler:\n",
    "                    with autocast():\n",
    "                        outputs = self.model(images)\n",
    "                        loss = self.criterion(outputs, labels)\n",
    "                    self.scaler.scale(loss).backward()\n",
    "                    self.scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), \n",
    "                                                  self.config.GRADIENT_CLIP)\n",
    "                    self.scaler.step(self.optimizer)\n",
    "                    self.scaler.update()\n",
    "                else:\n",
    "                    outputs = self.model(images)\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), \n",
    "                                                  self.config.GRADIENT_CLIP)\n",
    "                    self.optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{running_loss/(batch_idx+1):.4f}',\n",
    "                'acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        epoch_loss = running_loss / len(self.train_loader)\n",
    "        epoch_acc = 100. * correct / total\n",
    "        \n",
    "        return epoch_loss, epoch_acc\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def validate_epoch(self, epoch):\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "        \n",
    "        pbar = tqdm(self.val_loader, desc='Validation')\n",
    "        \n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(self.device), labels.to(self.device)\n",
    "            \n",
    "            if self.scaler:\n",
    "                with autocast():\n",
    "                    outputs = self.model(images)\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "            else:\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "            \n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "        \n",
    "        epoch_loss = running_loss / len(self.val_loader)\n",
    "        epoch_acc = 100. * accuracy_score(all_labels, all_preds)\n",
    "        epoch_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        return epoch_loss, epoch_acc, epoch_f1, all_preds, all_labels, all_probs\n",
    "    \n",
    "    def fit(self):\n",
    "        print(f\"\\nðŸš€ Starting Training on {self.device}\\n\")\n",
    "        \n",
    "        for epoch in range(self.config.NUM_EPOCHS):\n",
    "            # Train\n",
    "            train_loss, train_acc = self.train_epoch(epoch)\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_acc, val_f1, val_preds, val_labels, val_probs = \\\n",
    "                self.validate_epoch(epoch)\n",
    "            \n",
    "            # Update scheduler\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Log metrics\n",
    "            logger.log('train', epoch, {'loss': train_loss, 'acc': train_acc})\n",
    "            logger.log('val', epoch, {\n",
    "                'loss': val_loss, \n",
    "                'acc': val_acc, \n",
    "                'f1': val_f1\n",
    "            })\n",
    "            \n",
    "            # Save history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['val_acc'].append(val_acc)\n",
    "            self.history['val_f1'].append(val_f1)\n",
    "            \n",
    "            # Print summary\n",
    "            print(f\"\\n Epoch {epoch+1} Summary:\")\n",
    "            print(f\"   Train - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
    "            print(f\"   Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%, F1: {val_f1:.4f}\")\n",
    "            print(f\"   LR: {self.optimizer.param_groups[0]['lr']:.6f}\\n\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_acc > self.best_val_acc:\n",
    "                self.best_val_acc = val_acc\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'val_acc': val_acc,\n",
    "                    'val_f1': val_f1,\n",
    "                    'config': self.config.__dict__\n",
    "                }\n",
    "                torch.save(checkpoint, \n",
    "                          os.path.join(Config.CHECKPOINT_DIR, 'best_model.pth'))\n",
    "                print(f\"ðŸ’¾ New best model saved! Val Acc: {val_acc:.2f}%\\n\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if self.early_stopping(val_loss):\n",
    "                print(f\" Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        print(f\"\\n Training Complete! Best Val Acc: {self.best_val_acc:.2f}%\")\n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8c7be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 11: INITIALIZE & TRAIN\n",
    "# ==========================================\n",
    "# Create datasets\n",
    "train_dataset = AdvancedXRayDataset(\n",
    "    Config.BASE_DIR, 'train', \n",
    "    transform=get_train_transforms(Config.IMG_SIZE)\n",
    ")\n",
    "val_dataset = AdvancedXRayDataset(\n",
    "    Config.BASE_DIR, 'val',\n",
    "    transform=get_val_transforms(Config.IMG_SIZE)\n",
    ")\n",
    "test_dataset = AdvancedXRayDataset(\n",
    "    Config.BASE_DIR, 'test',\n",
    "    transform=get_val_transforms(Config.IMG_SIZE)\n",
    ")\n",
    "\n",
    "# Create loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, \n",
    "                         shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE,\n",
    "                       shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE,\n",
    "                        shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}\")\n",
    "\n",
    "# Initialize model\n",
    "model = AdvancedXRayModel(\n",
    "    model_name=Config.MODEL_NAME,\n",
    "    num_classes=Config.NUM_CLASSES,\n",
    "    pretrained=Config.PRETRAINED\n",
    ")\n",
    "\n",
    "print(f\" Model: {Config.MODEL_NAME}\")\n",
    "print(f\" Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Train\n",
    "trainer = Trainer(model, train_loader, val_loader, Config)\n",
    "history = trainer.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea8123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================== \n",
    "# SECTION 12: COMPREHENSIVE EVALUATION\n",
    "# ==========================================\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc='Testing'):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    # Metrics\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    print(f\"\\n Test Set Performance:\")\n",
    "    print(f\"   Accuracy: {acc*100:.2f}%\")\n",
    "    print(f\"   F1 Score: {f1:.4f}\\n\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\" Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, \n",
    "                                target_names=Config.CLASSES, digits=4))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=Config.CLASSES, yticklabels=Config.CLASSES)\n",
    "    plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(Config.LOG_DIR, 'confusion_matrix.png'), dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Per-class metrics\n",
    "    print(\"\\n Per-Class Accuracy:\")\n",
    "    class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "    for i, cls in enumerate(Config.CLASSES):\n",
    "        print(f\"   {cls}: {class_acc[i]*100:.2f}%\")\n",
    "    \n",
    "    # ROC curves\n",
    "    all_probs = np.array(all_probs)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, cls in enumerate(Config.CLASSES):\n",
    "        y_true = (np.array(all_labels) == i).astype(int)\n",
    "        y_score = all_probs[:, i]\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "        auc = roc_auc_score(y_true, y_score)\n",
    "        \n",
    "        axes[i].plot(fpr, tpr, linewidth=2, label=f'AUC = {auc:.4f}')\n",
    "        axes[i].plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "        axes[i].set_xlabel('False Positive Rate', fontsize=11)\n",
    "        axes[i].set_ylabel('True Positive Rate', fontsize=11)\n",
    "        axes[i].set_title(f'{cls} ROC Curve', fontsize=12, fontweight='bold')\n",
    "        axes[i].legend(loc='lower right', fontsize=10)\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(Config.LOG_DIR, 'roc_curves.png'), dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    return acc, f1, all_preds, all_labels, all_probs\n",
    "\n",
    "# Load best model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "checkpoint = torch.load(os.path.join(Config.CHECKPOINT_DIR, 'best_model.pth'))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate\n",
    "test_acc, test_f1, test_preds, test_labels, test_probs = evaluate_model(\n",
    "    model, test_loader, device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27423536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================== \n",
    "# SECTION 13: GRAD-CAM++ VISUALIZATION\n",
    "# ==========================================\n",
    "class GradCAMVisualizer:\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        target_layer = model.backbone.features[-1] \\\n",
    "            if hasattr(model.backbone, 'features') else model.backbone.layer4[-1]\n",
    "        self.cam = GradCAMPlusPlus(model=model, target_layers=[target_layer])\n",
    "    \n",
    "    def visualize_batch(self, images, labels, predictions, n=4):\n",
    "        fig, axes = plt.subplots(n, 3, figsize=(12, n*4))\n",
    "        \n",
    "        for i in range(min(n, len(images))):\n",
    "            img_tensor = images[i].unsqueeze(0).to(self.device)\n",
    "            \n",
    "            # Original\n",
    "            img = images[i].cpu().numpy().transpose(1, 2, 0)\n",
    "            img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "            img = np.clip(img, 0, 1)\n",
    "            \n",
    "            # GradCAM\n",
    "            grayscale_cam = self.cam(input_tensor=img_tensor, targets=[predictions[i].item()])\n",
    "            cam_image = show_cam_on_image(img, grayscale_cam[0], use_rgb=True)\n",
    "            \n",
    "            # Plot\n",
    "            axes[i, 0].imshow(img)\n",
    "            axes[i, 0].set_title(f'Original', fontsize=10)\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            axes[i, 1].imshow(cam_image)\n",
    "            axes[i, 1].set_title(f'GradCAM++', fontsize=10)\n",
    "            axes[i, 1].axis('off')\n",
    "            \n",
    "            axes[i, 2].imshow(grayscale_cam[0], cmap='jet')\n",
    "            axes[i, 2].set_title(f'True: {Config.CLASSES[labels[i]]}\\nPred: {Config.CLASSES[predictions[i]]}', \n",
    "                                fontsize=9)\n",
    "            axes[i, 2].axis('off')\n",
    "        \n",
    "        plt.suptitle('Model Interpretability - GradCAM++ Heatmaps', \n",
    "                    fontsize=14, fontweight='bold', y=1.00)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(Config.LOG_DIR, 'gradcam_visualization.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize\n",
    "visualizer = GradCAMVisualizer(model, device)\n",
    "sample_images, sample_labels = next(iter(test_loader))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(sample_images.to(device))\n",
    "    _, predictions = outputs.max(1)\n",
    "\n",
    "visualizer.visualize_batch(sample_images[:4], sample_labels[:4], predictions[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa754d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================== \n",
    "# SECTION 14: DEPLOYMENT PACKAGE\n",
    "# ==========================================\n",
    "print(\"\\n Creating deployment package...\")\n",
    "\n",
    "# Save model in multiple formats\n",
    "torch.save(model.state_dict(), os.path.join(Config.CHECKPOINT_DIR, 'model_weights.pth'))\n",
    "torch.save(model, os.path.join(Config.CHECKPOINT_DIR, 'model_complete.pth'))\n",
    "\n",
    "# Model info\n",
    "model_info = {\n",
    "    'model_name': Config.MODEL_NAME,\n",
    "    'accuracy': float(test_acc),\n",
    "    'f1_score': float(test_f1),\n",
    "    'classes': Config.CLASSES,\n",
    "    'image_size': Config.IMG_SIZE,\n",
    "    'parameters': sum(p.numel() for p in model.parameters()),\n",
    "    'training_date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    'config': {k: v for k, v in Config.__dict__.items() if not k.startswith('_')}\n",
    "}\n",
    "\n",
    "with open(os.path.join(Config.CHECKPOINT_DIR, 'model_info.json'), 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "# Create inference script\n",
    "inference_code = '''\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "def load_model(model_path, device='cpu'):\n",
    "    model = torch.load(model_path, map_location=device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def predict(model, image_path, device='cpu'):\n",
    "    transform = A.Compose([\n",
    "        A.Resize(256, 256),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    \n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = transform(image=image)['image'].unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        probs = F.softmax(output, dim=1)[0]\n",
    "        pred_idx = probs.argmax().item()\n",
    "        confidence = probs[pred_idx].item()\n",
    "    \n",
    "    classes = ['NORMAL', 'PNEUMONIA', 'COVID', 'TB']\n",
    "    return classes[pred_idx], confidence, probs.cpu().numpy()\n",
    "\n",
    "# Usage\n",
    "model = load_model('model_complete.pth')\n",
    "prediction, confidence, probabilities = predict(model, 'xray.jpg')\n",
    "print(f\"Prediction: {prediction} ({confidence*100:.2f}%)\")\n",
    "'''\n",
    "\n",
    "with open(os.path.join(Config.CHECKPOINT_DIR, 'inference.py'), 'w') as f:\n",
    "    f.write(inference_code)\n",
    "\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].plot(history['train_loss'], label='Train', marker='o', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Val', marker='s', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training History - Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history['train_acc'], label='Train', marker='o', linewidth=2)\n",
    "axes[1].plot(history['val_acc'], label='Val', marker='s', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Training History - Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(history['val_f1'], label='F1 Score', marker='d', \n",
    "            linewidth=2, color='green')\n",
    "axes[2].set_xlabel('Epoch', fontsize=12)\n",
    "axes[2].set_ylabel('F1 Score', fontsize=12)\n",
    "axes[2].set_title('Validation F1 Score', fontsize=14, fontweight='bold')\n",
    "axes[2].legend(fontsize=11)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(Config.LOG_DIR, 'training_history.png'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸŽ‰ TRAINING COMPLETE - FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Model: {Config.MODEL_NAME}\")\n",
    "print(f\" Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Total Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Training Time: {(time.time() - logger.start_time)/60:.1f} minutes\")\n",
    "print(f\"\\nSaved Files:\")\n",
    "print(f\"   Model: {Config.CHECKPOINT_DIR}/model_complete.pth\")\n",
    "print(f\"   Weights: {Config.CHECKPOINT_DIR}/model_weights.pth\")\n",
    "print(f\"   Metadata: {Config.CHECKPOINT_DIR}/model_info.json\")\n",
    "print(f\"   Inference Script: {Config.CHECKPOINT_DIR}/inference.py\")\n",
    "print(f\"   Logs: {Config.LOG_DIR}/\")\n",
    "print(\"=\"*70)\n",
    "print(\"Ready for deployment!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
