{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c38f14",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# SECTION 1: SETUP & INSTALLATIONS\n",
    "# ==========================================\n",
    "# Run this first - installs all required packages\n",
    "\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install kaggle opendatasets pillow matplotlib seaborn scikit-learn\n",
    "!pip install pytorch-grad-cam\n",
    "!pip install timm  # For better model architectures\n",
    "\n",
    "print(\" All packages installed!\")\n",
    "\n",
    "# ==========================================\n",
    "# SECTION 2: IMPORTS\n",
    "# ==========================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# ==========================================\n",
    "# SECTION 3: DATASET DOWNLOAD\n",
    "# ==========================================\n",
    "# This downloads ~5GB of data, takes 10-15 minutes\n",
    "\n",
    "import opendatasets as od\n",
    "\n",
    "# Dataset 1: Kaggle Chest X-Ray (Pneumonia)\n",
    "print(\"Downloading Pneumonia Dataset...\")\n",
    "od.download(\"https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia\")\n",
    "\n",
    "# Dataset 2: COVID-19 Radiography Database\n",
    "print(\"Downloading COVID-19 Dataset...\")\n",
    "od.download(\"https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database\")\n",
    "\n",
    "# Dataset 3: TB Chest X-ray Database (Alternative public source)\n",
    "print(\"Downloading TB Dataset...\")\n",
    "od.download(\"https://www.kaggle.com/datasets/raddar/tuberculosis-chest-xrays-shenzhen\")\n",
    "\n",
    "print(\" All datasets downloaded!\")\n",
    "\n",
    "# ==========================================\n",
    "# SECTION 4: DATA ORGANIZATION\n",
    "# ==========================================\n",
    "# Organize all datasets into a unified structure\n",
    "\n",
    "# Create organized directory structure\n",
    "base_dir = '/content/xray_dataset'\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    for category in ['NORMAL', 'PNEUMONIA', 'COVID', 'TB']:\n",
    "        os.makedirs(f'{base_dir}/{split}/{category}', exist_ok=True)\n",
    "\n",
    "print(\" Directory structure created!\")\n",
    "\n",
    "# Function to copy and organize images\n",
    "def organize_images(source_paths, dest_base, label, split_ratio={'train': 0.7, 'val': 0.15, 'test': 0.15}):\n",
    "    \"\"\"Organize images into train/val/test splits\"\"\"\n",
    "    all_images = []\n",
    "\n",
    "    for source_path in source_paths:\n",
    "        if os.path.exists(source_path):\n",
    "            images = [os.path.join(source_path, f) for f in os.listdir(source_path)\n",
    "                     if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "            all_images.extend(images)\n",
    "\n",
    "    # Shuffle\n",
    "    np.random.shuffle(all_images)\n",
    "\n",
    "    # Split\n",
    "    n_train = int(len(all_images) * split_ratio['train'])\n",
    "    n_val = int(len(all_images) * split_ratio['val'])\n",
    "\n",
    "    train_imgs = all_images[:n_train]\n",
    "    val_imgs = all_images[n_train:n_train + n_val]\n",
    "    test_imgs = all_images[n_train + n_val:]\n",
    "\n",
    "    # Copy files\n",
    "    for split, imgs in [('train', train_imgs), ('val', val_imgs), ('test', test_imgs)]:\n",
    "        dest_dir = f'{dest_base}/{split}/{label}'\n",
    "        for i, img_path in enumerate(tqdm(imgs, desc=f'{label} {split}')):\n",
    "            try:\n",
    "                shutil.copy(img_path, f'{dest_dir}/{label}_{split}_{i}.jpg')\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "    print(f\" {label}: Train={len(train_imgs)}, Val={len(val_imgs)}, Test={len(test_imgs)}\")\n",
    "\n",
    "# Organize NORMAL images (from pneumonia dataset)\n",
    "print(\" Organizing NORMAL images...\")\n",
    "organize_images(\n",
    "    ['/content/chest-xray-pneumonia/chest_xray/train/NORMAL',\n",
    "     '/content/chest-xray-pneumonia/chest_xray/test/NORMAL'],\n",
    "    base_dir, 'NORMAL'\n",
    ")\n",
    "\n",
    "# Organize PNEUMONIA images\n",
    "print(\" Organizing PNEUMONIA images...\")\n",
    "organize_images(\n",
    "    ['/content/chest-xray-pneumonia/chest_xray/train/PNEUMONIA',\n",
    "     '/content/chest-xray-pneumonia/chest_xray/test/PNEUMONIA'],\n",
    "    base_dir, 'PNEUMONIA'\n",
    ")\n",
    "\n",
    "# Organize COVID images\n",
    "print(\" Organizing COVID images...\")\n",
    "organize_images(\n",
    "    ['/content/covid19-radiography-database/COVID-19_Radiography_Dataset/COVID/images'],\n",
    "    base_dir, 'COVID'\n",
    ")\n",
    "\n",
    "# Organize TB images\n",
    "print(\" Organizing TB images...\")\n",
    "organize_images(\n",
    "    ['/content/tuberculosis-chest-xrays-shenzhen/images'], # Corrected path\n",
    "    base_dir, 'TB'\n",
    ")\n",
    "\n",
    "print(\" All data organized!\")\n",
    "\n",
    "# ==========================================\n",
    "# SECTION 5: DATA EXPLORATION\n",
    "# ==========================================\n",
    "\n",
    "# Count images in each category\n",
    "def count_images(base_path):\n",
    "    stats = {}\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        stats[split] = {}\n",
    "        for category in ['NORMAL', 'PNEUMONIA', 'COVID', 'TB']:\n",
    "            path = f'{base_path}/{split}/{category}'\n",
    "            stats[split][category] = len([f for f in os.listdir(path) if f.endswith('.jpg')])\n",
    "    return stats\n",
    "\n",
    "stats = count_images(base_dir)\n",
    "print(\" Dataset Statistics:\")\n",
    "print(pd.DataFrame(stats).T)\n",
    "\n",
    "# Visualize distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for idx, split in enumerate(['train', 'val', 'test']):\n",
    "    data = stats[split]\n",
    "    axes[idx].bar(data.keys(), data.values(), color=['green', 'blue', 'red', 'orange'])\n",
    "    axes[idx].set_title(f'{split.upper()} Set Distribution')\n",
    "    axes[idx].set_ylabel('Number of Images')\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# SECTION 6: DATASET CLASS\n",
    "# ==========================================\n",
    "\n",
    "class ChestXrayDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.classes = ['NORMAL', 'PNEUMONIA', 'COVID', 'TB']\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "\n",
    "        # Load all image paths and labels\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(root_dir, split, class_name)\n",
    "            if os.path.exists(class_dir):\n",
    "                for img_name in os.listdir(class_dir):\n",
    "                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        self.images.append(os.path.join(class_dir, img_name))\n",
    "                        self.labels.append(self.class_to_idx[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# ==========================================\n",
    "# SECTION 7: DATA AUGMENTATION & TRANSFORMS\n",
    "# ==========================================\n",
    "\n",
    "# Training transforms with augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Validation/Test transforms (no augmentation)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ChestXrayDataset(base_dir, split='train', transform=train_transform)\n",
    "val_dataset = ChestXrayDataset(base_dir, split='val', transform=val_transform)\n",
    "test_dataset = ChestXrayDataset(base_dir, split='test', transform=val_transform)\n",
    "\n",
    "print(f\" Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}\")\n",
    "\n",
    "# ==========================================\n",
    "# SECTION 8: DATA LOADERS\n",
    "# ==========================================\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(\" Data loaders created!\")\n",
    "\n",
    "# Visualize sample batch\n",
    "def show_batch(dataloader, n=8):\n",
    "    batch = next(iter(dataloader))\n",
    "    images, labels = batch\n",
    "\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    classes = ['NORMAL', 'PNEUMONIA', 'COVID', 'TB']\n",
    "\n",
    "    for i in range(min(n, len(images))):\n",
    "        img = images[i].cpu().numpy().transpose(1, 2, 0)\n",
    "        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "        img = np.clip(img, 0, 1)\n",
    "\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f\"Label: {classes[labels[i]]}\", fontsize=12)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\ Sample images from training set:\")\n",
    "show_batch(train_loader)\n",
    "\n",
    "# ==========================================\n",
    "# SECTION 9: MODEL ARCHITECTURE\n",
    "# ==========================================\n",
    "\n",
    "class XRayClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=4, pretrained=True):\n",
    "        super(XRayClassifier, self).__init__()\n",
    "\n",
    "        # Use EfficientNet-B0 (better than ResNet50 for medical images)\n",
    "        self.backbone = models.efficientnet_b0(pretrained=pretrained)\n",
    "\n",
    "        # Replace classifier\n",
    "        in_features = self.backbone.classifier[1].in_features\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = XRayClassifier(num_classes=4, pretrained=True).to(device)\n",
    "\n",
    "print(f\" Model created on {device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# ==========================================\n",
    "# SECTION 10: LOSS & OPTIMIZER\n",
    "# ==========================================\n",
    "\n",
    "# Calculate class weights for imbalanced dataset\n",
    "class_counts = [stats['train'][cls] for cls in ['NORMAL', 'PNEUMONIA', 'COVID', 'TB']]\n",
    "class_weights = torch.FloatTensor([1.0 / c if c > 0 else 0.0 for c in class_counts]) # Handle zero counts\n",
    "# If you intend to completely ignore classes with zero samples, you might further normalize only non-zero weights.\n",
    "# For now, setting to 0.0 will ensure no division by zero, but loss won't be impacted by this class.\n",
    "# A more robust solution might involve removing the class entirely if it has no samples.\n",
    "class_weights = class_weights / class_weights.sum() * 4  # Normalize\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "\n",
    "print(\" Loss function and optimizer configured!\")\n",
    "\n",
    "# ==========================================\n",
    "# SECTION 11: TRAINING FUNCTIONS\n",
    "# ==========================================\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(dataloader, desc='Training')\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        pbar.set_postfix({'loss': running_loss/len(dataloader), 'acc': 100.*correct/total})\n",
    "\n",
    "    return running_loss / len(dataloader), 100. * correct / total\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc='Validation')\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            pbar.set_postfix({'loss': running_loss/len(dataloader), 'acc': 100.*correct/total})\n",
    "\n",
    "    return running_loss / len(dataloader), 100. * correct / total, all_preds, all_labels\n",
    "\n",
    "# ==========================================\n",
    "# SECTION 12: TRAINING LOOP\n",
    "# ==========================================\n",
    "\n",
    "num_epochs = 20\n",
    "best_val_acc = 0.0\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "print(\"\\nðŸš€ Starting Training...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "\n",
    "    # Validate\n",
    "    val_loss, val_acc, val_preds, val_labels = validate_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "    # Update scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "        }, 'best_xray_model.pth')\n",
    "        print(f\"New best model saved! Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    print()\n",
    "\n",
    "print(f\" Training Complete! Best Val Accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "# ==========================================\n",
    "# SECTION 13: PLOT TRAINING HISTORY\n",
    "# ==========================================\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "ax1.plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot(history['train_acc'], label='Train Acc', marker='o')\n",
    "ax2.plot(history['val_acc'], label='Val Acc', marker='s')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# SECTION 14: TEST EVALUATION\n",
    "# ==========================================\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load('best_xray_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(\" Best model loaded!\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc, test_preds, test_labels = validate_epoch(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\" Test Set Performance:\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "# Classification Report\n",
    "classes = ['NORMAL', 'PNEUMONIA', 'COVID', 'TB']\n",
    "print(\" Classification Report:\")\n",
    "print(classification_report(test_labels, test_preds, target_names=classes))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "plt.title('Confusion Matrix - Test Set')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "for i, cls in enumerate(classes):\n",
    "    print(f\"{cls} Accuracy: {class_accuracy[i]*100:.2f}%\")\n",
    "\n",
    "# ==========================================\n",
    "# SECTION 15: EXPORT MODEL\n",
    "# ==========================================\n",
    "\n",
    "# Export for deployment\n",
    "torch.save(model.state_dict(), 'xray_model_weights.pth')\n",
    "torch.save(model, 'xray_model_complete.pth')\n",
    "\n",
    "# Save model info\n",
    "model_info = {\n",
    "    'accuracy': test_acc,\n",
    "    'classes': classes,\n",
    "    'input_size': (224, 224),\n",
    "    'num_parameters': sum(p.numel() for p in model.parameters())\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('model_info.json', 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(\" Model exported successfully!\")\n",
    "print(\"Files saved:\")\n",
    "print(\"  - best_xray_model.pth (checkpoint)\")\n",
    "print(\"  - xray_model_weights.pth (weights only)\")\n",
    "print(\"  - xray_model_complete.pth (complete model)\")\n",
    "print(\"  - model_info.json (metadata)\")\n",
    "\n",
    "# ==========================================\n",
    "# SECTION 16: GRADCAM IMPLEMENTATION\n",
    "# ==========================================\n",
    "\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "\n",
    "class GradCAMVisualizer:\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        # Target the last convolutional layer\n",
    "        target_layer = model.backbone.features[-1]\n",
    "        self.cam = GradCAM(model=model, target_layers=[target_layer])\n",
    "\n",
    "    def generate_heatmap(self, image_tensor, predicted_class):\n",
    "        \"\"\"Generate GradCAM heatmap for an image\"\"\"\n",
    "        grayscale_cam = self.cam(input_tensor=image_tensor.unsqueeze(0),\n",
    "                                  targets=[predicted_class])\n",
    "        return grayscale_cam[0]\n",
    "\n",
    "# Test GradCAM\n",
    "gradcam = GradCAMVisualizer(model, device)\n",
    "\n",
    "# Get sample images\n",
    "sample_images, sample_labels = next(iter(test_loader))\n",
    "sample_images = sample_images[:4].to(device)\n",
    "sample_labels = sample_labels[:4]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(sample_images)\n",
    "    _, predictions = outputs.max(1)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i in range(4):\n",
    "    # Original image\n",
    "    img = sample_images[i].cpu().numpy().transpose(1, 2, 0)\n",
    "    img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "    img = np.clip(img, 0, 1)\n",
    "\n",
    "    # Generate heatmap\n",
    "    heatmap = gradcam.generate_heatmap(sample_images[i], predictions[i].item())\n",
    "\n",
    "    # Overlay\n",
    "    cam_image = show_cam_on_image(img, heatmap, use_rgb=True)\n",
    "\n",
    "    # Plot\n",
    "    axes[0, i].imshow(img)\n",
    "    axes[0, i].set_title(f\"True: {classes[sample_labels[i]]}\", fontsize=10)\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "    axes[1, i].imshow(cam_image)\n",
    "    axes[1, i].set_title(f\"Pred: {classes[predictions[i]]}\", fontsize=10)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('GradCAM Visualizations - Red areas show where AI is looking', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"GradCAM visualization complete!\")\n",
    "\n",
    "# ==========================================\n",
    "# SECTION 17: INFERENCE FUNCTION\n",
    "# ==========================================\n",
    "\n",
    "def predict_xray(model, image_path, device, transform):\n",
    "    \"\"\"\n",
    "    Predict disease from X-ray image\n",
    "\n",
    "    Returns:\n",
    "        prediction: class name\n",
    "        confidence: probability score\n",
    "        heatmap: GradCAM visualization\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        confidence, predicted = probabilities.max(1)\n",
    "\n",
    "    # Get class name\n",
    "    classes = ['NORMAL', 'PNEUMONIA', 'COVID', 'TB']\n",
    "    prediction = classes[predicted.item()]\n",
    "    confidence_score = confidence.item() * 100\n",
    "\n",
    "    # Generate heatmap\n",
    "    gradcam_viz = GradCAMVisualizer(model, device)\n",
    "    heatmap = gradcam_viz.generate_heatmap(image_tensor[0], predicted.item())\n",
    "\n",
    "    return prediction, confidence_score, heatmap\n",
    "\n",
    "# Test inference\n",
    "test_image_path = test_dataset.images[0]\n",
    "prediction, confidence, heatmap = predict_xray(model, test_image_path, device, val_transform)\n",
    "\n",
    "print(f\" Inference Test:\")\n",
    "print(f\"Prediction: {prediction}\")\n",
    "print(f\"Confidence: {confidence:.2f}%\")\n",
    "\n",
    "# ==========================================\n",
    "# SECTION 18: SAVE FOR DEPLOYMENT\n",
    "# ==========================================\n",
    "\n",
    "# Create deployment package\n",
    "import zipfile\n",
    "\n",
    "deployment_files = [\n",
    "    'best_xray_model.pth',\n",
    "    'xray_model_weights.pth',\n",
    "    'xray_model_complete.pth',\n",
    "    'model_info.json'\n",
    "]\n",
    "\n",
    "with zipfile.ZipFile('xray_model_deployment.zip', 'w') as zipf:\n",
    "    for file in deployment_files:\n",
    "        if os.path.exists(file):\n",
    "            zipf.write(file)\n",
    "\n",
    "print(\" Deployment package created: xray_model_deployment.zip\")\n",
    "print(\" ALL DONE! Your model is ready for deployment!\")\n",
    "print(f\" Final Metrics:\")\n",
    "print(f\"   Test Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"   Model Size: ~{os.path.getsize('xray_model_complete.pth') / (1024*1024):.1f} MB\")\n",
    "print(f\"   Classes: {', '.join(classes)}\")\n",
    "print(f\" Download the model files to deploy in your backend!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
